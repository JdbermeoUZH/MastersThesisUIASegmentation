tta_mode: dae_and_diffusion # no_tta, gnn, gt, dae, dae_and_diffusion

start_new_exp: False
wandb_log: True

dataset: hcp_t2   # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
split:   val       # val, test
image_size: [1, 256, 256]
start: null       # Index of the test dataset to start at
stop:  null       # Index of the test dataset to stop at

seg_dir: /scratch_net/biwidl319/jbermeo/data/models/brain/segmentation/hcp_t1/no_bg_supp_norm_w_3x3_conv
load_best_cpt: True

num_workers: 0
device: cuda
seed: 0

bg_suppression_opts:
  type: none  # possible values: none, fixed_value, random_value
  bg_value: -0.5
  bg_value_min: -0.5
  bg_value_max: 1
  mask_source: thresholding  # possible values: thresholding, ground_truth
  thresholding: otsu  # possible values: isodata, li, mean, minimum, otsu, triangle, yen
  hole_filling: True


dae:
  wandb_project: mt-tta-brain # mt-tta-brain-hcp_t2
  logdir: /scratch_net/biwidl319/jbermeo/logs/brain/tta/debugging/dae_only/3_01/lr_1em3_grad_acc_1x1_conv_norm
  dae_dir: /scratch_net/biwidl319/jbermeo/data/models/brain/dae/hcp_t1
  
  seg_with_bg_supp: False

  accumulate_over_volume: True
  batch_size: 16
  dataset_repetition: 1
  num_steps: 1000  # 500 for brain, 2700 for wmh
  update_dae_output_every: 25
  calculate_dice_every: 25  # 25 for brain, 135 for wmh (nuhs, umc, vu)
  alpha: 1.0
  beta: 0.25
  learning_rate: 0.001
  save_checkpoints: True
  const_aug_per_volume: False
  
  bg_suppression_opts:
    type: none  # possible values: none, fixed_value, random_value
    bg_value: -0.5
    bg_value_min: -0.5
    bg_value_max: 1
    mask_source: thresholding  # possible values: thresholding, ground_truth
    thresholding: otsu  # possible values: isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling: True
  
  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.1


dae_and_ddpm:
  wandb_project: mt-tta-brain # mt-tta-brain-hcp_t2
  logdir: /scratch_net/biwidl319/jbermeo/logs/brain/tta/tests/4_17
  dae_dir: /scratch_net/biwidl319/jbermeo/data/models/brain/dae/hcp_t1
  ddpm_dir: /scratch_net/biwidl319/jbermeo/logs/brain/ddpm/hcp_t1/normalized_imgs/3x3_norm_filters/4_26/batch_size_130_dim_64_dim_mults_1_2_2_2_cond_by_concatenation_with_unconditional_training_rate_0.75
  cpt_fn: 'model-6.pt'
  
  # Loss weights
  dae_loss_alpha: 1.0
  ddpm_loss_beta: 1.0
  ddpm_uncond_loss_gamma: 0.0
  x_norm_regularization_eta: 0.0

  # DDPM loss params
  use_adaptive_beta: True
  adaptive_beta_momentum: 0.8
  classifier_free_guidance_weight: 100      # 30, or 100
  use_y_pred_for_ddpm_loss: True            # If False, and use_y_gt_for_ddpm_loss is False, then it will use the pseudo label from the DAE
  use_y_gt_for_ddpm_loss: False             # Only true for debugging
  detach_x_norm_from_ddpm_loss: False       # Use only gradients on the segmentation

  # Loss functions
  ddpm_loss: sds                            # jacobian, ssd, dds, pds 
  x_norm_regularization_loss: sq_grad       # null, sq_grad, zncc, rsq_sift, sift, mi 

  # Finetune batch normalization layers
  finetune_bn: False
  track_running_stats_bn: True
  subset_bn_layers: null                    # [0, 1]. If null, it will use all the layers

  # DAE term parameters  
  update_dae_output_every: 200              # 25 for brain, 135 for wmh (nuhs, umc, vu). Try 200 for brain and 270 for wmh for more stable convergence
  calculate_dice_every: 50    
  alpha: 1.0
  beta: 0.25 
  use_atlas_only_for_init: False

  # DDPM params
  min_max_intenities_norm_imgs: [-0.7377170324325562, 0.24169665575027466]
  update_norm_td_statistics: True
  unconditional_rate: 0                     # It is the rate at which it does forward passes in unconditional mode
  #   switch and warmup
  use_ddpm_after_step: null                 # 500. If null, this condition is not used
  use_ddpm_after_dice: null                 # 0.5. If null, this condition is not used
  warmup_steps_for_ddpm_loss: 100           # 100. If null, no warmup is used
  #   minibatching
  minibatch_size_ddpm: 4
  frac_vol_diffusion_tta: 1.0
  #   sampling range for t's
  t_ddpm_range: [0.02, 0.98]                # [0.02, 0.98] 
  t_sampling_strategy: uniform              # uniform, stratified, one_per_volume
  
  sampling_timesteps: 100

  # Optimization parameters 
  num_steps: 2000                           # 500 for brain, 2700 for wmh
  learning_rate: 0.001
  accumulate_over_volume: True
  batch_size: 32
  dataset_repetition: 1
  save_checkpoints: True
  const_aug_per_volume: False

  # Normalize intensities before segmentation
  manually_norm_img_before_seg_val: False
  manually_norm_img_before_seg_tta: False
  normalization_strategy: standardize       # standardize, min_max, null
  min_max_quantile: [0.1, 0.975]
    
  # Supress background x_norm 
  bg_supp_x_norm: False
  bg_suppression_opts:
    type: fixed_value                              # none, fixed_value, random_value
    bg_value: -0.5
    bg_value_min: -0.5
    bg_value_max: 1
    mask_source: thresholding               # thresholding, ground_truth
    thresholding: otsu                      # isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling: True
  
  # Augmentation
  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.0

diffusion_tta:
  wandb_project: mt-tta-diffusion_tta-tests
  logdir: /scratch_net/biwidl319/jbermeo/logs/brain/sota/diffusion_tta/tests/original_algo
  ddpm_dir: /scratch_net/biwidl319/jbermeo/logs/brain/ddpm/original_imgs/not_one_hot_64_base_filters_with_aug_except_noise
  cpt_fn: 'model-9.pt' # or 23, 28, or 31

  original_algo: True
  batch_size: 4
  minibatch_size_ddpm: 2
  batch_size_ddpm: 1 #180 # 180 for original algo
  num_steps:  1 # 5   # 5 for original algo
  
  learning_rate: 0.00001
  learning_rate_norm: null # If null, takes the same value as learning_rate
  learning_rate_seg: null  # If null, takes the same value as learning_rate
  learning_rate_ddpm: null # If null, takes the same value as learning_rate

  fit_norm_params: True
  fit_seg_params: True
  fit_ddpm_params: True

  frac_vol_diffusion_tta: 0.25
  min_t_diffusion_tta: 0
  max_t_diffusion_tta: 1000
  sampling_timesteps: 100
  min_max_int_norm_imgs: [0, 1]
  
  calculate_dice_every: 25  # 25 for brain, 135 for wmh (nuhs, umc, vu)
  accumulate_over_volume: False
  save_checkpoints: True
  const_aug_per_volume: False
  
  bg_suppression_opts:
    type: fixed_value  # possible values: none, fixed_value, random_value
    bg_value: -0.5
    bg_value_min: -0.5
    bg_value_max: 1
    mask_source: thresholding  # possible values: thresholding, ground_truth
    thresholding: otsu  # possible values: isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling: True
  
  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.0