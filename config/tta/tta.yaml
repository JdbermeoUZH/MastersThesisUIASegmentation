tta_mode: null                # Typically defined when running a tta type: no_tta, gnn, gt, dae, dae_and_diffusion
continuous_tta: false         # If false, TTA is run for each volume (episodically). 

start_new_exp: False
wandb_log: True
wandb_run_name:   null
debug_mode: True
print_config: True

dataset: hcp_t2   # subcortical_structures: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
dataset_type: Normal          # Normal, DinoFeatures
split:   test                 # val, test
eval_orientation: depth       # depth, width, or height
load_in_memory: True          # If True, it will load the entire dataset in memory
dataset_mode: 3D              # 3D, 2D
load_dataset_in_memory: False
node_data_path: /scratch/${USER}/data # If not null, it copies the data to the compute node (doesn't work on precalculated features, too big)


start: null                   # Index of the test dataset to start at
stop:  null                   # Index of the test dataset to stop at

seg_dir: $MODEL_DIR/subcortical_structures/segmentation/hcp_t1/no_bg_supp_norm_w_3x3_conv
load_best_cpt: True
classes_of_interest: null

num_workers: 2
device: cuda
seed: 0

viz_interm_outs: ['Normalized Image']
save_predicted_vol_as_nifti: True

no_tta:
  wandb_project: baseline_performance 
  logdir: $RESULTS_DIR/subcortical_structures/tta/debugging/no_tta/9_06/reproduce_previous_results

  batch_size: 64
    
dae:
  wandb_project: tta-dae # mt-tta-subcortical_structures-hcp_t2
  logdir: $RESULTS_DIR/subcortical_structures/tta/debugging/dae_only/9_06/reproduce_previous_results
  dae_dir: $MODEL_DIR/subcortical_structures/dae/hcp_t1

  alpha: 1.0
  beta: 0.25
  use_only_dae_pl: false
  use_only_atlas: false
  update_dae_output_every: 50

  fit_at_test_time: "normalizer" # Options: bn_layers, all, normalizer
  accumulate_over_volume: True
  batch_size: 32
  num_steps: 1000 #1000  # 500 for subcortical_structures, 2700 for wmh
  calculate_dice_every: 25  # 25 for subcortical_structures, 135 for wmh (nuhs, umc, vu)
  learning_rate: 0.001
  max_grad_norm: null
  save_checkpoints: True
  const_aug_per_volume: False

  # loss function parameters
  smooth: 0
  epsilon: 1e-10
  
  # Supress background x_norm 
  bg_supp_x_norm_tta_dae: False
  bg_suppression_opts:
    type: fixed_value  # possible values: fixed_value, random_value, (none if not used)
    bg_value: -0.5
    bg_value_min: -0.5
    bg_value_max: 1
    mask_source: thresholding  # possible values: thresholding, ground_truth
    thresholding: otsu  # possible values: isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling: True
  
  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.1


dae_and_ddpm:
  wandb_project: mt-tta-subcortical_structures # mt-tta-subcortical_structures-hcp_t2
  logdir: $RESULTS_DIR/subcortical_structures/tta/tests/4_17
  dae_dir: $MODEL_DIR/subcortical_structures/dae/hcp_t1
  ddpm_dir: $MODEL_DIR/subcortical_structures/ddpm/hcp_t1/normalized_imgs/3x3_norm_filters/4_26/batch_size_130_dim_64_dim_mults_1_2_2_2_cond_by_concatenation_with_unconditional_training_rate_0.75
  cpt_fn: 'model-6.pt'
  
  # Loss weights
  dae_loss_alpha: 1.0
  ddpm_loss_beta: 1.0
  ddpm_uncond_loss_gamma: 0.0
  x_norm_regularization_eta: 1.0

  # DDPM loss params
  use_adaptive_beta: False
  adaptive_beta_momentum: 0.8
  classifier_free_guidance_weight: 100      # 30, or 100
  use_y_pred_for_ddpm_loss: True            # If False, and use_y_gt_for_ddpm_loss is False, then it will use the pseudo label from the DAE
  use_y_gt_for_ddpm_loss: False             # Only true for debugging
  detach_x_norm_from_ddpm_loss: False       # Use only gradients on the segmentation
  ddpm_loss_only_on_classes: null

  # Loss functions
  ddpm_loss: jacobian                            # jacobian, ssd, dds, pds 
  x_norm_regularization_loss: sq_grad       # null, sq_grad, zncc, rsq_sift, sift, mi 

  # Finetune batch normalization layers
  finetune_bn: False
  track_running_stats_bn: True
  subset_bn_layers: null                    # [0, 1]. If null, it will use all the layers

  # DAE term parameters  
  update_dae_output_every: 200              # 25 for subcortical_structures, 135 for wmh (nuhs, umc, vu). Try 200 for subcortical_structures and 270 for wmh for more stable convergence
  calculate_dice_every: 50    
  alpha: 1.0
  beta: 0.25 
  use_atlas_only_for_init: False
  dae_loss_except_on_classes: null

  # DDPM params
  min_max_intenities_norm_imgs: [-0.7627120614051819, 0.26643848419189453]
  update_norm_td_statistics: True
  unconditional_rate: 0                     # It is the rate at which it does forward passes in unconditional mode
  #   switch and warmup
  use_ddpm_after_step: null                 # 500. If null, this condition is not used
  use_ddpm_after_dice: null                 # 0.5. If null, this condition is not used
  warmup_steps_for_ddpm_loss: 100           # 100. If null, no warmup is used
  #   minibatching
  minibatch_size_ddpm: 4
  frac_vol_diffusion_tta: 1.0
  #   sampling range for t's
  t_ddpm_range: [0.02, 0.98]                # [0.02, 0.98] 
  t_sampling_strategy: uniform              # uniform, stratified, one_per_volume
  
  sampling_timesteps: 100

  # Optimization parameters 
  num_steps: 2000                           # 500 for subcortical_structures, 2700 for wmh
  learning_rate: 0.001
  accumulate_over_volume: True
  batch_size: 32
  dataset_repetition: 1
  save_checkpoints: True
  const_aug_per_volume: False

  # Normalize intensities before segmentation
  manually_norm_img_before_seg_val: True
  manually_norm_img_before_seg_tta: False
  normalization_strategy: standardize       # standardize, min_max, null
  min_max_quantile: [0.025, 0.975]
  classes_of_interest: null                 # 16 for wmh + synthseg, null for subcortical_structures and wmh  

  # Supress background x_norm 
  bg_supp_x_norm_dae: True
  bg_supp_x_norm_ddpm: False
  bg_supp_x_norm_eval: False
  bg_suppression_opts:
    type: fixed_value                       # none, fixed_value, random_value
    bg_value: -0.5
    bg_value_min: -0.5
    bg_value_max: 1
    mask_source: thresholding               # thresholding, ground_truth
    thresholding: otsu                      # isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling: True
  
  # Augmentation
  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.0

diffusionTTA:
  wandb_project: mt-tta-diffusionTTA-tests
  logdir: $RESULTS_DIR/wmh/tta/nuhs/test/diffusionTTA/06_12
  ddpm_dir: $MODEL_DIR/subcortical_structures/ddpm/hcp_t1/original_sd_imgs/06_09/batch_size_130_dim_64_dim_mults_1_2_2_2_uncond_rate_0.2_imgsize_128x128_no_class_balancing_norm_q_0.001_0.999
  cpt_fn: 'model-6.pt'

  num_steps:  20                           # 5 for original algo (it accumulates gradients over the entire volume for entire batch size)
  num_t_noise_pairs_per_img: 180
  batch_size: 256                         # Used for eval and during training
  minibatch_size_ddpm: 16                  # 8 for small GPUs, 32-64 for A6000
  dataset_repetition: 1
  const_aug_per_volume: True

  learning_rate: 0.00008                   # 8e-5
  learning_rate_norm: null                 # If null, takes the same value as learning_rate
  learning_rate_seg: null                  # If null, takes the same value as learning_rate
  learning_rate_ddpm: null                 # If null, takes the same value as learning_rate

  fit_norm_params: True
  fit_seg_params: True
  fit_ddpm_params: True

  ddpm_loss: jacobian                      # jacobian, ssd, dds, pds. Jacobian for original algo
  w_cfg: 0                                 # 0.0 for original algo 
  min_max_intenities_norm_imgs: [0, 1]
  pair_sampling_type: one_per_volume       # 'one_per_volume', 'one_per_image'
  t_ddpm_range: [0.02, 0.98]               # [0.02, 0.98] 
  t_sampling_strategy: uniform             # uniform, stratified
  unconditional_rate: 0                    # It is the rate at which it does forward passes in unconditional mode

  save_checkpoints: True
  calculate_dice_every: 1                  # We take very few steps, so we calculate the dice every step
  classes_of_interest: null                  # 16 for wmh + synthseg, null for subcortical_structures and wmh  
  
  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.0


diffusionTTA_and_DAE:
  wandb_project: mt-tta-diffusionTTA-tests
  logdir: $RESULTS_DIR/subcortical_structures/tta/hcp_t2/test/diffusionTTA_and_DAE/06_12
  ddpm_dir: $MODEL_DIR/subcortical_structures/ddpm/hcp_t1/original_sd_imgs/06_09/batch_size_130_dim_64_dim_mults_1_2_2_2_uncond_rate_0.2_imgsize_128x128_no_class_balancing_norm_q_0.001_0.999
  cpt_fn: 'model-6.pt'
  dae_dir: $MODEL_DIR/subcortical_structures/dae/hcp_t1

  num_steps:  1000                        # 5 for original algo (it accumulates gradients over the entire volume for entire batch size)
  batch_size: 32                          # Used for eval and during training
  minibatch_size_ddpm: 8                  # 8 for small GPUs, 64 for A6000
  dataset_repetition: 1
  const_aug_per_volume: True

  dae_loss_alpha: 1.0 
  ddpm_loss_beta: 1.0

  learning_rate: 0.00008                   # 8e-5
  learning_rate_norm: null                 # If null, takes the same value as learning_rate
  learning_rate_seg: null                  # If null, takes the same value as learning_rate
  learning_rate_ddpm: null                 # If null, takes the same value as learning_rate
  accumulate_over_volume: True

  fit_norm_params: True
  fit_seg_params: True
  fit_ddpm_params: True

  save_checkpoints: True
  calculate_dice_every: 50                 # 25 for subcortical_structures, 135 for wmh (nuhs, umc, vu)
  classes_of_interest: null                  # 16 for wmh + synthseg, null for subcortical_structures and wmh  

  # DAE Params
  update_dae_output_every: 100
  alpha: 1.0
  beta: 0.25
  bg_suppression_opts:
    type: fixed_value                       # none, fixed_value, random_value
    bg_value: -0.5
    bg_value_min: -0.5
    bg_value_max: 1
    mask_source: thresholding               # thresholding, ground_truth
    thresholding: otsu                      # isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling: True
  
  # DDPM Params
  num_t_noise_pairs_per_img: 1             # 180 used in original algo
  ddpm_loss: jacobian                      # jacobian, ssd, dds, pds. Jacobian for original algo
  w_cfg: 100                               # 0.0 for original algo 
  min_max_intenities_norm_imgs: [0, 1]
  pair_sampling_type: one_per_volume       # 'one_per_volume', 'one_per_image'
  t_ddpm_range: [0.02, 0.98]               # [0.02, 0.98] 
  t_sampling_strategy: uniform             # uniform, stratified
  unconditional_rate: 0                    # It is the rate at which it does forward passes in unconditional mode

  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.0


entropy_min:
  wandb_project: tta-entropy_min 
  logdir: $RESULTS_DIR/subcortical_structures/tta/debugging/entropy_min/12_16/DELETE_ME

  # Loss function
  class_prior_type: uniform # uniform, data
  use_kl_loss: true
  kl_loss_weight: 1.0
  weighted_loss: true

  clases_to_exclude_ent_term: null
  classes_to_exclude_kl_term: null #[0]

  filter_low_support_classes: false

  # Optimization parameters
  num_steps: 500

  fit_at_test_time: bn_layers  # Options: bn_layers, all, normalizer

  learning_rate: 0.000001
  weight_decay: 0.001
  
  batch_size: 32
  gradient_acc_steps: 1

  lr_decay: True
  lr_scheduler_step_size: 20
  lr_scheduler_gamma: 0.7

  evaluate_every: 25

  augmentation:
    da_ratio: 0.25
    sigma: 20
    alpha: 0
    trans_min: 0
    trans_max: 0
    rot_min: 0
    rot_max: 0
    scale_min: 1.0
    scale_max: 1.0
    gamma_min: 0.5
    gamma_max: 2.0
    brightness_min: 0.0
    brightness_max: 0.1
    noise_mean: 0.0
    noise_std: 0.1

  