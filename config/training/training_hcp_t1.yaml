wandb_log:        True
start_new_exp:    False
seed:             0
resume:           False
device:           cuda
checkpoint_best:  checkpoint_best.pth
checkpoint_last:  checkpoint_last.pth
n_classes:        15     # 15 for brain, 2 for WMH

dae:
  wandb_project:                mt-tta-dae-dae_training
  batch_size:                   1
  num_workers:                  3
  dataset:                      hcp_t1               # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: umc, nuhs, vu
  image_size:                   [256, 256, 256]    # [256, 256, 256] for brain, [48, 256, 256] for wmh
  rescale_factor:               [0.25, 1, 1]         # [0.25, 1, 1] for brain, [1, 1, 1] for wmh
  epochs:                       2500              # 2500 for brain (hcp_t1, hcp_t2), 5000 for brain (abide_caltech) and WMH (nuhs, umc, vu)
  validate_every:               50               # 50 for brain (hcp_t1, hcp_t2), 100 for brain (abide_caltech) and WMH (nuhs, umc, vu)
  learning_rate:                1.0e-3
  validation_set_multiplier:    50
  logdir:                       /scratch_net/biwidl319/jbermeo/data/models/brain/dae/hcp_t1
  
  augmentation:
    da_ratio:             0.25
    sigma:                20
    alpha:                1000
    trans_min:            -10
    trans_max:            10
    rot_min:              -10
    rot_max:              10
    scale_min:            0.9
    scale_max:            1.1
    gamma_min:            1.0
    gamma_max:            1.0
    brightness_min:       0.0
    brightness_max:       0.0
    noise_mean:           0.0
    noise_std:            0.0
  
  deformation:
    mask_type:            squares_jigsaw # zeros / random_labels / jigsaw
    mask_radius:          10 # The mask will be a square with side length twice this number 
    mask_squares:         200
    is_num_masks_fixed:   False
    is_size_masks_fixed:  False

segmentation:
  batch_size:                   16
  num_workers:                  3
  dataset:                      hcp_t1  # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  image_size:                   [1, 256, 256]
  epochs:                       150  # 150 for brain, 1000 for WMH (nuhs, umc)
  validate_every:               3
  learning_rate:                0.001
  logdir:                       /scratch_net/biwidl319/jbermeo/data/models/brain/segmentation/hcp_t1/no_bg_supp_norm_w_3x3_conv
  wandb_project:                mt-segmentation_models
  
  with_bg_supression:           False

  bg_suppression_opts:
    type:               none  # possible values: none, fixed_value, random_value
    bg_value:           -0.5
    bg_value_min:       -0.5
    bg_value_max:       1
    mask_source:        thresholding  # possible values: thresholding, ground_truth
    thresholding:       otsu  # possible values: isodata, li, mean, minimum, otsu, triangle, yen
    hole_filling:       True
  
  augmentation:
    da_ratio:           0.25
    sigma:              20
    alpha:              1000
    trans_min:          -10
    trans_max:          10
    rot_min:            -10
    rot_max:            10
    scale_min:          0.9
    scale_max:          1.1
    gamma_min:          0.5 #0.2     # Default 0.5
    gamma_max:          2.0 #5.0     # Default 2.0
    brightness_min:     0.0 #-0.2    # Default 0.0
    brightness_max:     0.1 #0.2     # Default 0.1
    noise_mean:         0.0
    noise_std:          0.1


ddpm:
  batch_size:                   8
  gradient_accumulate_every:    16
  num_workers:                  3
  dataset:                      hcp_t1    # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  norm_with_nn_on_fly:          True
  use_original_imgs:            False
  one_hot_encode:               True
  normalize:                    min_max
  image_size:                   [1, 256, 256]
  rescale_factor:               [1, 1, 1]
  condition_by_mult:            False
  also_unconditional:           False
  only_unconditional:           False
  unconditional_rate:           0.2

  objective:                    pred_v   # default is pred_v
  learning_rate:                8e-5     # 1e-4 in the constructor
  amp:                          True
  train_num_steps:              50000    # 7,000,000 or 100,000 in the original script
  timesteps:                    1000     # Range of noising steps to sample from during training. 1000 in the constructor  
  sampling_timesteps:           100      # Number of sampling steps to generate images. If less than 1000, will use DDIM sampling. If null, will be the same as timesteps => linear denoising steps
  save_and_sample_every:        2500     # 1000 in the constructor 
  num_validation_samples:       100      # Num of images to sample to check performance during training
  num_viz_samples:              50       # Must have an interger square root to store images in a square grid
  log_val_loss_every:           250  

  logdir:                       /scratch_net/biwidl319/jbermeo/logs/brain/ddpm/hcp_t1/normalized_imgs/3x3_norm_filters/4_15/batch_size_128_cond_by_concat
  wandb_project:                mt-ddpm
  
  norm_dir:                     /scratch_net/biwidl319/jbermeo/data/models/brain/segmentation/hcp_t1/no_bg_supp_norm_w_3x3_conv
  load_best_cpt:                True
  
  augmentation:                 
    da_ratio:           0.25
    sigma:              20
    alpha:              1000
    trans_min:          -10
    trans_max:          10
    rot_min:            -10
    rot_max:            10
    scale_min:          0.9
    scale_max:          1.1
    gamma_min:          0.5     # Default 0.5
    gamma_max:          2.0     # Default 2.0
    brightness_min:     0.0     # Default 0.0
    brightness_max:     0.1     # Default 0.1
    noise_mean:         0.0
    noise_std:          0.0

  bg_suppression_opts:          null
  deformation:                  null

  invalid_img:
    prob:               0.0
    bad_contrast:
      prob:             0.0
      gamma_min:        0.01
      gamma_max:        0.2 
    blurry:
      prob:             0.0
      blur_factor:      0.5
  
    invalid_labels:
      prob:             0.0
      jigsaw_cropping:    
        prob:           0.0
        mask_radius:    10
        mask_squares:   200
        is_num_masks_fixed:   False
        is_size_masks_fixed:  False
      random_pixel_erasure:
        prob:           0.0
        erasure_prob:   0.5
        erasure_size:   10
      random_label_erasure:
        prob:           0.0
        erasure_prob:   0.5
      random_component_erasure:
        prob:           0.0
        erasure_prob:   0.5
      random_label_mask:
        prob:           0.0
        random_noise_p: 0.5
        random_shapes_p: 0.5

ddpm_oai:
  logdir:                       /scratch_net/biwidl319/jbermeo/logs/brain/ddpm/hcp_t1/oai/normalized_imgs/tests/test_img_logging
  wandb_project:                mt-ddpm

  seg_cond:                     True  
  noise_schedule:               cosine    # linear of cosine
  dropout:                      0.3       # Use 0.1 for linear schedule, and 0.3 for cosine schedule
  learn_sigma:                  False
  use_kl:                       False     # Use L_VLB loss (also use t_0 and t_T of the KL divergence)
  schedule_sampler:             uniform   # 'loss-second-moment' or 'uniform'
  
  train_num_steps:              50000     # 200,000 in the original script
  learning_rate:                1e-4      # 1e-4 in the constructor
  batch_size:                   128
  microbatch:                   8         # -1 disables microbatching
  num_workers:                  3
  use_fp16:                     False
  log_interval:                 2         # Default 10
  save_interval:                2500      # Default 10000
  num_samples_for_metrics:      100       # Must have an interger square root to store images in a square grid
  diffusion_steps:              4000      # Range of noising steps to sample from during training. 4000 in the original implementation  
  use_ddim:                     True      # To generate samples to check performance during training
  timestep_respacing:           ddim100   # Number of sampling steps to generate images. If prepended with ddim, then it uses ddim style respacing

  dataset:                      umc       # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  split:                        train
  split_val:                    val
  norm_with_nn_on_fly:          True
  norm_dir:                     /scratch_net/biwidl319/jbermeo/data/models/wmh/segmentation/umc/no_bg_supp_norm_w_3x3_conv
  load_best_cpt:                True
  norm_device:                  cpu
  use_original_imgs:            False
  one_hot_encode:               True
  normalize:                    min_max
  image_size:                   [1, 256, 256]
  rescale_factor:               [1, 1, 1]
    
  augmentation:                 
    da_ratio:           0.25
    sigma:              20
    alpha:              1000
    trans_min:          -10
    trans_max:          10
    rot_min:            -10
    rot_max:            10
    scale_min:          0.9
    scale_max:          1.1
    gamma_min:          0.5
    gamma_max:          2.0
    brightness_min:     0.0
    brightness_max:     0.1
    noise_mean:         0.0
    noise_std:          0.0

  bg_suppression_opts:          null
  deformation:                  null