wandb_log:        False
seed:             0
resume:           False
device:           cuda
checkpoint_best:  checkpoint_best.pth
checkpoint_last:  checkpoint_last.pth
n_classes:        15                # 15 for brain, 2 for WMH

dae:
  wandb_project:                mt-tta-dae-dae_training
  batch_size:                   1
  num_workers:                  3
  dataset:                      hcp_t1            # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  image_size:                   [256, 256, 256]   # [256, 256, 256] for brain, [48, 256, 256] for wmh
  rescale_factor:               [0.25, 1, 1]      # [0.25, 1, 1] for brain, [1, 1, 1] for wmh
  epochs:                       2500              # 2500 for brain (hcp_t1, hcp_t2), 5000 for brain (abide_caltech) and WMH (nuhs, umc, vu)
  validate_every:               50                # 50 for brain (hcp_t1, hcp_t2), 100 for brain (abide_caltech) and WMH (nuhs, umc, vu)
  learning_rate:                1.0e-3
  validation_set_multiplier:    50
  logdir:                       /scratch_net/biwidl319/jbermeo/logs/brain/dae/hcp_t1
  
  augmentation:
    da_ratio:             0.25
    sigma:                20
    alpha:                1000
    trans_min:            -10
    trans_max:            10
    rot_min:              -10
    rot_max:              10
    scale_min:            0.9
    scale_max:            1.1
    gamma_min:            1.0
    gamma_max:            1.0
    brightness_min:       0.0
    brightness_max:       0.0
    noise_mean:           0.0
    noise_std:            0.0
  
  deformation:
    mask_type:            squares_jigsaw # zeros / random_labels / jigsaw
    mask_radius:          10 # The mask will be a square with side length twice this number 
    mask_squares:         200
    is_num_masks_fixed:   False
    is_size_masks_fixed:  False

segmentation:
  batch_size:                   16
  num_workers:                  3
  dataset:                      hcp_t1  # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  image_size:                   [1, 256, 256]
  epochs:                       150  # 150 for brain, 1000 for WMH (nuhs, umc)
  validate_every:               3
  learning_rate:                1.0e-3
  logdir:                       /scratch_net/biwidl319/jbermeo/data/models/brain/segmentation/hcp_t1/no_bg_supp_norm_w_3x3_conv
  wandb_project:                mt-segmentation_models
  
  with_bg_supression:           False
  bg_suppression_opts:
      type:               none  # possible values: none, fixed_value, random_value
      bg_value:           -0.5
      bg_value_min:       -0.5
      bg_value_max:       1
      mask_source:        thresholding  # possible values: thresholding, ground_truth
      thresholding:       otsu  # possible values: isodata, li, mean, minimum, otsu, triangle, yen
      hole_filling:       True
  
  augmentation:
      da_ratio:           0.25
      sigma:              20
      alpha:              1000
      trans_min:          -10
      trans_max:          10
      rot_min:            -10
      rot_max:            10
      scale_min:          0.9
      scale_max:          1.1
      gamma_min:          0.5
      gamma_max:          2.0
      brightness_min:     0.0
      brightness_max:     0.1
      noise_mean:         0.0
      noise_std:          0.1


ddpm:
  batch_size:                   2
  gradient_accumulate_every:    8
  num_workers:                  5
  dataset:                      hcp_t1    # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  norm_with_nn_on_fly:          False
  use_original_imgs:            True
  one_hot_encode:               False
  normalize:                    min_max
  image_size:                   [1, 256, 256]
  rescale_factor:               [1, 1, 1]
  train_num_steps:              50000    # 7,000,000 or 100,000 in the original script
  num_samples :                 25        # Must have an interger square root to store images in a square grid
  timesteps:                    1000      # Range of noising steps to sample from during training. 1000 in the constructor  
  sampling_timesteps:           100       # Number of sampling steps to generate images. If less than 1000, will use DDIM sampling. If null, will be the same as timesteps => linear denoising steps
  save_and_sample_every:        2500      # 1000 in the constructor 
  learning_rate:                8e-5      # 1e-4 in the constructor
  logdir:                       /scratch_net/biwidl319/jbermeo/logs/brain/ddpm/original_imgs/not_one_hot_64_base_filters_with_aug_except_noise
  wandb_project:                mt-ddpm

  norm_dir:                     /scratch_net/biwidl319/jbermeo/data/models/brain/segmentation/hcp_t1/no_bg_supp_norm_w_1x1_conv

  augmentation:                 
    da_ratio:           0.25
    sigma:              20
    alpha:              1000
    trans_min:          -10
    trans_max:          10
    rot_min:            -10
    rot_max:            10
    scale_min:          0.9
    scale_max:          1.1
    gamma_min:          0.5
    gamma_max:          2.0
    brightness_min:     0.0
    brightness_max:     0.1
    noise_mean:         0.0
    noise_std:          0.0

  bg_suppression_opts:          null
  deformation:                  null


ddpm_oai:
  logdir:                       /scratch_net/biwidl319/jbermeo/logs/brain/ddpm/oai/normalized_imgs/03_012/L_simple_test_single_gpu_2
  wandb_project:                mt-ddpm

  seg_cond:                     True  
  learn_sigma:                  False
  noise_schedule:               cosine    # linear of cosine
  use_kl:                       False     # Use L_VLB loss (also use t_0 and t_T of the KL divergence)
  schedule_sampler:             uniform   # 'loss-second-moment' or 'uniform'
  
  train_num_steps:              50000     # 200,000 in the original script
  learning_rate:                1e-4      # 1e-4 in the constructor
  batch_size:                   4
  microbatch:                   2         # -1 disables microbatching
  num_workers:                  2
  log_interval:                 10        # Default 10
  save_interval:                2500      # Default 10000
  num_samples_for_metrics:      250       # Must have an interger square root to store images in a square grid
  diffusion_steps:              4000      # Range of noising steps to sample from during training. 4000 in the original implementation  
  use_ddim:                     True      # To generate samples to check performance during training
  timestep_respacing:           ddim100   # Number of sampling steps to generate images. If prepended with ddim, then it uses ddim style respacing

  dataset:                      hcp_t1    # brain: hcp_t1, hcp_t2, abide_caltech, abide_stanford; wmh: nuhs, umc, vu
  split:                        train
  split_val:                    val
  norm_with_nn_on_fly:          True
  norm_dir:                     /scratch_net/biwidl319/jbermeo/data/models/brain/segmentation/hcp_t1/no_bg_supp_norm_w_1x1_conv
  norm_device:                  cpu
  use_original_imgs:            False
  one_hot_encode:               True
  normalize:                    min_max
  image_size:                   [1, 256, 256]
  rescale_factor:               [1, 1, 1]
    
  augmentation:                 
    da_ratio:           0.25
    sigma:              20
    alpha:              1000
    trans_min:          -10
    trans_max:          10
    rot_min:            -10
    rot_max:            10
    scale_min:          0.9
    scale_max:          1.1
    gamma_min:          0.5
    gamma_max:          2.0
    brightness_min:     0.0
    brightness_max:     0.1
    noise_mean:         0.0
    noise_std:          0.0

  bg_suppression_opts:          null
  deformation:                  null